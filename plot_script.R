# This is an R script used to analyze some of the arxiv-metadata
# data frames exported from the export_data.py file.  Please run that
# first, make the csv files, then run this script in the directory
# you saved the csv files to.

#setwd("pathway goes here") 
#getwd() 
# uncomment these lines above
# changes the directory, depends on your machine

library(ggplot2) # install ggplot2
library(ggrepel) # install ggrepel


# import the dataframes from the csv files.  These csv files are generated by
# export_data.py.
topic_year <- read.csv('topic_year.csv')
word_topic <- read.csv('word_topic.csv')
word_year <- read.csv('word_year.csv')

# some early years have very few data points, so we delete those
topic_year <- subset(topic_year, YEAR_TOTAL > 10000)
word_year <- subset(word_year, YEAR_TOTAL > 10000)

# now we want to get rid of some stop words
stop_words = c('and', 'with', 'the', 'a', 'of', 'in', 'for', 'on', 'to', 'from',
               'an', 'at', 'by', 'using', 'for.', 'in.')

word_year <- word_year[,!names(word_year) %in% stop_words]
word_topic <- word_topic[,!names(word_topic) %in% stop_words]

################################################################################

# One question we can answer with this metadata: which arxiv categories
# appear the more as primary categories for a paper, and which appear more as 
# secondary topics.  The arxiv requires a topic that the paper is listed under
# and can be crossed referenced to others.  We can see which one happens more
# by comparing the FIRST_CAT_TOTAL column, which measures primary topic number,
# and the CAT_MENTION_TOTAL column, which counts just the number of times the
# topic is listed as a paper category, primary or not.

# Here is a plot of the total mentions of a topic vs the number of times
# it is the primary topic of a paper.  Since a primary topic counts as a mention,
# then the data lie above the y=x diagonal.  But still there is a linear relationship.
ggplot(word_topic, aes(x=FIRST_CAT_TOTAL,y=CAT_MENTION_TOTAL,)) +
  labs(x='Primary Topic Mentions', y='Total Topic Mentions') +
  geom_point(col='blue') +
  geom_smooth(method='lm', formula= y~x, col='red')

# Taking the best fit line and looking at the coefficients, we see that for every 100
# papers with primary topic X, there are approximately 140 papers listed with
# that topic total.
x <- c(word_topic$FIRST_CAT_TOTAL)
y <- c(word_topic$CAT_MENTION_TOTAL)
lm(y~x)$coefficients

# Some of the outliers occur when there are 0 mentions of the topic primarily,
# but perhaps this is due to a quirk of the arxiv categorizing system, rather
# than an actual appraisal of academic fields. It is a reasonable assumption,
# that if a paper has a crossover subject, then that subject exists in and of itself.
# So we would hope that the best fit line goes through the origin. 
# We will remove these 4 points and see how it affects the best line fit through the origin.
temp_df <- word_topic[c('FIRST_CAT_TOTAL', 'CAT_MENTION_TOTAL')]
temp_df <- subset(temp_df, FIRST_CAT_TOTAL > 0)
ggplot(temp_df, aes(x=FIRST_CAT_TOTAL,y=CAT_MENTION_TOTAL,)) +
  labs(x='Primary Topic Mentions', y='Total Topic Mentions') +
  geom_point(col='blue') +
  geom_smooth(method='lm', formula= y~0+x, col='red')
x <- c(temp_df$FIRST_CAT_TOTAL)
y <- c(temp_df$CAT_MENTION_TOTAL)
lm(y ~ 0 + x)$coefficients

# Actually insisting that the y-int = 0 means we can ignore the x = 0
# points in the regression, but removing them visually is helpful too.


# The new slope is 1.485, so there are about 48-49 extra papers per 100 papers with
# a primary topic.  In other words, almost 33% of papers in a topic mentions are
# secondary.


# We can also identify the subjects which are outliers, i.e. have many more
# total topic mentions than primary topic mentions.

temp_df <- word_topic[c('FIRST_CAT_TOTAL', 'CAT_MENTION_TOTAL', 'TOKEN_LABEL')]
temp_df <- subset(temp_df, FIRST_CAT_TOTAL > 0)
ggplot(temp_df, aes(x=FIRST_CAT_TOTAL,y=CAT_MENTION_TOTAL)) +
  labs(x='Primary Topic Mentions', y='Total Topic Mentions') +
  geom_point(col='blue') +
  geom_text_repel(data=subset(temp_df,
                        abs(CAT_MENTION_TOTAL - 1.485*FIRST_CAT_TOTAL) > 
                          8000),aes(FIRST_CAT_TOTAL, CAT_MENTION_TOTAL,label=TOKEN_LABEL)) + 
  geom_smooth(method='lm', formula= y~0+x, col='red')

# Ironically, cs.LG corresponds to Machine Learning.  Seems like many people are
# jumping on the machine learning bandwagon from other disciplines and
# putting out machine learning papers.  You can also see this from the cs.AI
# and the stat.ML outliers.  

# Let us see how these topics have been used over the years out of curiosity.
temp_df <- data.frame(x=topic_year$TOKEN_LABEL,
                      y=c(topic_year$cs.LG/topic_year$YEAR_TOTAL,
                          topic_year$cs.AI/topic_year$YEAR_TOTAL,
                          topic_year$stat.ML/topic_year$YEAR_TOTAL),
                      group = c(rep('cs.LG', nrow(topic_year)),
                                rep('cs.AI', nrow(topic_year)),
                                rep('stat.ML', nrow(topic_year))))

ggplot(temp_df, aes(x, y, col=group)) + 
  geom_line(linewidth=0.5) +
  geom_point(size=2) +
  scale_color_discrete(name='') +
  theme_bw()

# Naturally all of them spiked since the early 2010s, but interesting that
# stat.ML became less popular around 2018 as cs.AI increased.  Perhaps it moved
# more into computer science at that point, but more context would be needed.
  

################################################################################

# Question: Which words/concepts have trended upwards in on the arxiv the last 2
# decades?  We can again try to answer this questions using word appearances
# in titles.  There are many words so this data is a bit noisy, but we can see
# what we get.

# We'll measure upward trend by relative mentions, as percentage of total papers
# since more papers have been uploaded to the arxiv in recent years 
# than in the 90's and early 00s.

percent_word_year <- cbind(word_year[c(1,2,3)],
                           word_year[-c(1,2,3)]/word_year$YEAR_TOTAL)

# We can try to measure popularity growth over the decades by 
# largest single year increase in percentage usage, and
# highest total increase.  For the highest total increase, we can
# use the absolute number since all papers are increasing. Since 2023 isn't
# over, we'll go from 1994 to 2022.

# a function which calculates the highest single year increase
highest_increase <- function(vector) {
  
  vector1 <- vector[-length(vector)]
  vector2 <- vector[-1]
  
  diff_vector <- vector2 - vector1
  
  max_slope <- max(diff_vector)
  
  return(max_slope)
}
highest_increase(c(1,2,3,5,1,10,130,-1))


# a function which calculates the total increase, without the final vector entry
total_increase <- function(vector) {
  average_slope <- (vector[length(vector)-1] - vector[1])
  return(average_slope)
}
total_increase(c(1,2,3,5,1,10,130,-1))


# makes a data frame of words with their respective measure
make_measure_df <- function(measure, name, frame, number_to_ignore) {
  
  temp_df <- data.frame(
    words = list(),
    name = c()
  )

  for (i in number_to_ignore+1:(length(frame)-number_to_ignore)) {
    word <- colnames(frame)[i]
    number <- measure(frame[[i]])
    temp_df <- rbind(temp_df, c(word, number))
  }
  
  colnames(temp_df)[1] = "words"
  colnames(temp_df)[2] = name

  return(temp_df)
}


# making data frame with highest single year increase data
highest_increase_df <- make_measure_df(highest_increase,
                                      'highest_increase',
                                      percent_word_year,
                                      3)
highest_increase_df$highest_increase <- as.numeric(
  as.character(highest_increase_df$highest_increase)) # changing to numeric
View(highest_increase_df)


# making data frame with highest total increase data
total_increase_df <- make_measure_df(total_increase,
                                      'total_increase',
                                      word_year,
                                      3)
total_increase_df$total_increase <- as.numeric(
  as.character(total_increase_df$total_increase)) # changing to numeric
View(total_increase_df)


# Now we can plot results for the top 5 in each respect.  Naturally,
# the highest increase measures biggest jump, which may not be the same
# as high overall trend.

df <- highest_increase_df
df <- df[order(df$highest_increase, decreasing=TRUE),]
high_words = df$words[1:5]
print(high_words)

# Now that we can see which words came out on top, we can manually make the plot.

group = c()
for (word in high_words) {
  addon <- c(rep(word, nrow(word_year)))
  group <- append(group,addon)
}

temp_df <- data.frame(x=word_year$TOKEN_LABEL,
                      y=c(percent_word_year$learning,
                          percent_word_year$covid.19,
                          percent_word_year$deep,
                          percent_word_year$states,
                          percent_word_year$matrix),
                      group=group)

ggplot(temp_df, aes(x, y, col=group)) + 
  geom_line(linewidth=0.5) +
  geom_point(size=2) +
  scale_color_discrete(name='') +
  theme_bw()

# Actually came out to be a nice-ish plot.  We can see deep learning really
# takes off around 2017.  The words 'states' and 'matrix' have
# noisy jumps in the early years.  'Covid-19' has an obvious jump in 2020.

# Let's try this with total change over from 1994 - 2022.

rm(df, temp_df)

df <- total_increase_df
df <- df[order(df$total_increase, decreasing=TRUE),]
high_words = df$words[1:5]
print(high_words)

group = c()
for (word in high_words) {
  addon <- c(rep(word, nrow(word_year)-1))
  group <- append(group,addon)
}

temp_df <- data.frame(x=word_year$TOKEN_LABEL[-30],
                      y=c(word_year$learning[-30],
                          word_year$quantum[-30],
                          word_year$model[-30],
                          word_year$neural[-30],
                          word_year$via[-30]),
                      group=group)

ggplot(temp_df, aes(x, y, col=group)) + 
  geom_line(linewidth=0.5) +
  geom_point(size=2) +
  scale_color_discrete(name='') +
  theme_bw()

# Again a pretty good graph.  Learning is really taking off , but we have some
# new additions with steady increase.  The words 'model' and 'neural'
# follow the same trend, but not as strong as 'learning.  'quantum' is consistent
# probably because physicists use that adjective all the time.
# 'Via' is a little surprising since it is a preposition.  
# Perhaps more and more researchers are proving known facts 'via' some 
# other method, and putting that in the title.  
# Or there is another reason I'm missing.














